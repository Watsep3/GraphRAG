import torch
import numpy as np
import pandas as pd
from typing import Dict, List
import json
from tqdm import tqdm
import sys
sys.path.append('C:/Projects/GraphRAG/src')

from rag.rag_pipeline import GraphRAGPipeline
from cross_modal_training import CrossModalProjection, CrossModalTrainer

class GraphRAGEvaluator:
    """√âvaluateur pour GraphRAG"""
    
    def __init__(self, pipeline: GraphRAGPipeline):
        self.pipeline = pipeline
    
    def evaluate_retrieval(self, questions: List[str], 
                          ground_truth_entities: List[List[str]],
                          k: int = 5) -> Dict:
        """
        √âvalue la qualit√© du retrieval
        
        Args:
            questions: Liste de questions
            ground_truth_entities: Liste de listes d'entit√©s pertinentes
            k: Top-K pour les m√©triques
        """
        
        metrics = {
            'precision@k': [],
            'recall@k': [],
            'f1@k': [],
            'mrr': []  # Mean Reciprocal Rank
        }
        
        for question, gt_entities in tqdm(zip(questions, ground_truth_entities), 
                                          total=len(questions),
                                          desc="Evaluation"):
            
            # R√©cup√©rer les r√©sultats
            results = self.pipeline.retriever.hybrid_search(question, k_text=k)
            retrieved = [r['entity'] for r in results['text_results']]
            
            # Precision@K
            relevant_in_k = len(set(retrieved[:k]) & set(gt_entities))
            precision = relevant_in_k / k if k > 0 else 0
            metrics['precision@k'].append(precision)
            
            # Recall@K
            recall = relevant_in_k / len(gt_entities) if len(gt_entities) > 0 else 0
            metrics['recall@k'].append(recall)
            
            # F1@K
            if precision + recall > 0:
                f1 = 2 * (precision * recall) / (precision + recall)
            else:
                f1 = 0
            metrics['f1@k'].append(f1)
            
            # MRR
            for i, entity in enumerate(retrieved, 1):
                if entity in gt_entities:
                    metrics['mrr'].append(1 / i)
                    break
            else:
                metrics['mrr'].append(0)
        
        # Moyennes
        return {
            f'precision@{k}': np.mean(metrics['precision@k']),
            f'recall@{k}': np.mean(metrics['recall@k']),
            f'f1@{k}': np.mean(metrics['f1@k']),
            'mrr': np.mean(metrics['mrr'])
        }
    
    def evaluate_qa(self, qa_pairs: List[Dict]) -> Dict:
        """
        √âvalue sur des paires question-r√©ponse
        
        Args:
            qa_pairs: Liste de {'question': str, 'answer': str}
        """
        
        # Pour l'instant, √©valuation simple bas√©e sur la pr√©sence d'entit√©s
        scores = []
        
        for qa in tqdm(qa_pairs, desc="QA Evaluation"):
            question = qa['question']
            expected_answer = qa['answer']
            
            # Obtenir la r√©ponse du syst√®me
            result = self.pipeline.query(question, k_text=5)
            
            # Score simple: pr√©sence de mots-cl√©s
            answer_words = set(expected_answer.lower().split())
            result_words = set(result['answer'].lower().split())
            
            overlap = len(answer_words & result_words)
            score = overlap / len(answer_words) if len(answer_words) > 0 else 0
            
            scores.append(score)
        
        return {
            'avg_word_overlap': np.mean(scores),
            'median_word_overlap': np.median(scores)
        }
    
    def benchmark(self, test_file: str, k: int = 5) -> Dict:
        """
        Benchmark complet sur un fichier de test
        
        Format du fichier:
        [
            {
                "question": "...",
                "answer": "...",
                "entities": ["entity1", "entity2", ...]
            },
            ...
        ]
        """
        
        print(f"Chargement du benchmark: {test_file}")
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        questions = [item['question'] for item in test_data]
        ground_truth_entities = [item.get('entities', []) for item in test_data]
        
        # √âvaluation retrieval
        print("\n1. √âvaluation Retrieval...")
        retrieval_metrics = self.evaluate_retrieval(questions, ground_truth_entities, k=k)
        
        # √âvaluation QA
        print("\n2. √âvaluation QA...")
        qa_metrics = self.evaluate_qa(test_data)
        
        # Combiner
        all_metrics = {**retrieval_metrics, **qa_metrics}
        
        return all_metrics

def create_synthetic_benchmark(output_file: str, n_samples: int = 100):
    """
    Cr√©e un benchmark synth√©tique pour tester
    """
    
    print(f"Cr√©ation de {n_samples} exemples synth√©tiques...")
    
    # Charger les entit√©s disponibles
    import pickle
    with open('C:/Projects/GraphRAG/models/embeddings/entity_embeddings.pkl', 'rb') as f:
        entity_data = pickle.load(f)
    
    entities = entity_data['name'].tolist()[:1000]
    
    benchmark = []
    
    for i in range(n_samples):
        # S√©lectionner des entit√©s al√©atoires
        selected = np.random.choice(entities, size=3, replace=False)
        
        # Cr√©er une question synth√©tique
        question = f"Tell me about {selected[0]} and its relation to {selected[1]}"
        answer = f"{selected[0]} is related to {selected[1]} through {selected[2]}"
        
        benchmark.append({
            'id': i,
            'question': question,
            'answer': answer,
            'entities': list(selected)
        })
    
    # Sauvegarder
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark, f, indent=2, ensure_ascii=False)
    
    print(f"‚úì Benchmark sauvegard√©: {output_file}")
    return benchmark

# Script principal
def main():
    print("="*60)
    print("√âVALUATION GRAPHRAG")
    print("="*60)
    
    # 1. Cr√©er un benchmark synth√©tique
    benchmark_file = 'C:/Projects/GraphRAG/data/processed/synthetic_benchmark.json'
    
    if not os.path.exists(benchmark_file):
        create_synthetic_benchmark(benchmark_file, n_samples=50)
    
    # 2. Initialiser le pipeline
    print("\n1. Initialisation du pipeline...")
    pipeline = GraphRAGPipeline(
        embeddings_path='C:/Projects/GraphRAG/models/embeddings/entity_embeddings.pkl'
    )
    
    # 3. Cr√©er l'√©valuateur
    evaluator = GraphRAGEvaluator(pipeline)
    
    # 4. Ex√©cuter le benchmark
    print("\n2. Ex√©cution du benchmark...")
    metrics = evaluator.benchmark(benchmark_file, k=5)
    
    # 5. Afficher les r√©sultats
    print("\n" + "="*60)
    print("R√âSULTATS D'√âVALUATION")
    print("="*60)
    
    for metric, value in metrics.items():
        print(f"{metric:.<40} {value:.4f}")
    
    # 6. Sauvegarder les r√©sultats
    results_file = 'C:/Projects/GraphRAG/results/evaluation_results.json'
    os.makedirs('C:/Projects/GraphRAG/results', exist_ok=True)
    
    with open(results_file, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"\n‚úì R√©sultats sauvegard√©s: {results_file}")
    
    # 7. Exemples de requ√™tes
    print("\n" + "="*60)
    print("EXEMPLES DE REQU√äTES")
    print("="*60)
    
    test_queries = [
        "What is machine learning?",
        "Who is Barack Obama?",
        "Explain neural networks"
    ]
    
    for query in test_queries:
        print(f"\nüìù Query: {query}")
        result = pipeline.query(query, k_text=3)
        print(f"Entities: {', '.join(result['entities'][:5])}")
    
    pipeline.close()
    
    print("\n‚úì‚úì‚úì √âVALUATION TERMIN√âE ‚úì‚úì‚úì")

if __name__ == "__main__":
    import os
    main()